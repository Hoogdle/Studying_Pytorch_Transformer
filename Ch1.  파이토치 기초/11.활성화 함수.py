### 활성화함수(Activation Function)
# : 은닉층을 활성화하기 위한 함수 
# : 네트워크가 데이터의 복잡한 패턴을 기반으로 학습하고 결정을 내릴 수 있게 제어
# : 선형에서 비선형으로 갱신하는 역할
# : 비선형적 변환을 통해 역전파가 가능하게 함(선형적이라면 미분시 상수 값이 나와 학습 진행이 어렵다)
# : 함수 입력을 정규화 하는 과정(정규화 : 데이터를 특정범위로 변환하여 범위를 일치시키는 것)

### 활성화
# : 인공 신경망의 뉴런의 출력값을 비선형으로 변환하는 것.

### 이진 분류(Logistic Regression==Logistic Classification)
# : 이진 분류란 규칙에 따라 입력된 값을 두 그룹으로 분류하는 작업.
# : 치역은 0(False)와 1(True) 만이 존재
# : 참 또는 거짓, A그룹 또는 B그룹
# : 임계값은 0.5로 설정됌
# : 0.5미만 => False, 0.5초과 => True

# 하지만 값은 0과1이 아닌 0과 1사이의 값도 가능하다! ====> Sigmoid Function


### 시그모이드 함수(Sigmoid Function)
# : 비선형적 
# : 0~1 또는 -1~1 사이의 반환값을 갖는다.
# : x의 계수(값 X)에 따라 Sigmoid 함수의 경사가 달라진다.(x의 계수가 작을수록 급격)
# : 로지스틱 회귀에 사용(독립변수의 선형결합을 이용해 종속변수를 범주형 데이터로 계산, 회귀 및 분류에서도 사용할 수 있음)
# : 시그모이드를 분류로 사용한다면 0.5보다 낮으면 False 0.5보다 크면 True 로 분류
# : 유연한 미분값 => 입력에 따른 값이 급격하게 변하지 않는다.
# : '기울기 폭주' 문제는 발생하지 않지만 '기울기 소실'문제가 발생할 수 있다.(역전파 과정 중 시그모이드 함수를 계속 미분하게 되는데 시그모이드 함수의 기울기 최대값(x=0)은 0.25밖에 되지 않고 나머지 값들은 매우 작다. 따라)
# 역전파 과정 중 시그모이드 함수를 계속 미분하게 되는데 시그모이드 함수의 기울기 최대값(x=0)은 0.25밖에 되지 않고 나머지 값들은 매우 작다. 따라서 시그모이드 미분 값들이 계속 곱해지게 되면 값이 작아질 수 밖에 없다.(기울기 소실)


### 이진 교차 엔트로피(Binary Cross Entropy, BCE)
# : 이진 분류에서 사용
# : 평균제곱오차(MSE)의 단점을 보완(예측값과 실제값의 차이가 작으면 계산되는 오차 또한 작아진다
# : ex) 예측값 : 0.0001 실제값: 1, MSE=> loss = (0.0001-1)^2 , 다르게 예측했는데도 loss 값이 크지 않다.... ==> 이진 교차 엔트로피
# : 실제값과 다르게 예측한 경우 loss 값을 매우크게 부여할 수 있다! 