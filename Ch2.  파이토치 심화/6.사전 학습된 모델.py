### 사전 학습된 모델
# 대규모 데이터세트로 학습된 딥러닝 모델로 이미 학습이 완료된 모델
# 사전 학습된 모델 자체를 현재 시스템에 적용하거나 사전 학습된 임베딩 벡터를 활용해 모델을 구성할 수 있다.
# 사전 학습된 모델을 활용하면 처음부터 모델을 구성하고 학습하는 것이 아닌, 이미 학습된 모델의 일부를 활용하거나 추가 학습을 통해 모델의 성능을 끌어낼 수 있다.
# ex) 늑대와 사자를 구별하는 모델을 구축하는 경우
# 처음부터 모델을 학습하지 않고 개와 고양이를 구별하는 사전 학습된 모델을 활용해 모델을 구축한다.
# 이미 개와 고양이가 가진 특징을 구분하는 계층에 대한 학습이 완료되어 있을 것이고 각 계층의 가중치만 수정해 늑대와 사자를 구분할 수 있다.


### 백본(Backbone)
# 입력 데이터에서 특징을 추출해 최종 분류기에 전달하는 딥러닝 모델 또는 딥러닝 모델의 일부.
# 입력 데이터에서 특징을 추출하므로 불필요한 특성을 제거하고 가장 중요한 특징을 추출할 수 있다. 추출된 특징을 활용해 새로운 모델이나 기능의 입력으로 사용한다.
# ex) 합성곱 신경망에서 백본의 활용 예시
# 이미지에서 객체를 검출하는 합성곱 신경망은 초기(하위)계층 에서는 점과 선 같은 저수준의 특징을 학습, 중간 계층에서는 객체나 형태를 학습, 최종(상위)계층에서는 이전 계층의 특징을 기반으로 객체를 이해하고 검출한다.
# 이 때 객체 검출 모델이 아닌, 포즈 추정 모델이나 이미지 분할 모델로 확장하려고 한다면 모델을 처음부터 구성하는 것이 아닌 최종 계층을 바꿔 기존 모델과 다른 모델을 구성할 수 있다.
# 자연어 처리와 컴퓨티 비전 작업에서 백본이 되는 모델은 BERT,GPT,VGG-16,ResNet 과 같은 초대규모 딥러닝 모델을 사용한다.


### 전이학습(Transfer Learning)
# 어떤 작업을 수행하기 전에 이미 사전 학습된 모델을 재사용해 새로운 작업이나 관련 도메인의 성능을 향상시킬 수 있는 기술
# 특정 영역의 대규모 데이트세트로 사전 학습된 모델을 다른 영역의 작은 데이터세트로 미세 조정해 활용
# 개와 고양이 구분 모델로 늑대 사자를 검출하는 모델을 만든 것과 유사
# 유사한 특징영역(눈,코,입)을 학습하여 소스 도메인에서 학습한 지식을 활용해 타깃 도메인에서 모델의 성능을 향상시키는 것
# 즉, 사전 학습된 모델을 활용해 현재 시스템에 맞는 새로운 모델로 학습하는 과정
# 업스트림(UpStream)모델 : 전이 학습을 수행하기 위해 사전 학습된 모델, 대규모 특정 도메인의 데이터세트에서 학습됨.
# 다운스트림(Downstream)모델 : 미세 조정된 모델, 업스트림 모델에서 학습한 지식을 활용해 작은 규모의 타깃 도메인 데이터세트에서 학습한 모델, 사전 학습된 모델의 계층을 하나 이상 사용.
# 귀납적 전이 학습,변환적 전이 학습, 비지도 전이 학습 등이 있따.


### 귀납적 전이 학습(Inductive Transfer Learning)
# 기존에 학습한 모델의 지식을 활용하여 새로운 작업을 수행하기 위한 방법 중 하나.
# 자기주도적 학습(Self-taught Learning)과 다중 작업 학습(Multi-task Learning) 으로 나뉨

# 자기주도적 학습
# 비지도 전이 학습의 유형 중 하나
# 소스 도메인의 데이터세트에서 데이터의 양은 많으나 레이블링된 데이터의 수가 매우 적거나 없을 때 사용

# 다중 작업 학습
# 지정된 소스 도메인과 타깃 도메인 데이터를 기반으로 모델에 여러 작업을 동시에 가르치는 방법
# 모델 구조는 공유 계층(Shared Layers)과 작업별계층(Task Specific Layer) 으로 나뉜다

# 공유 계층
# 소스 도메인과 타깃 도메인의 데이터세트로 모델을 사전 학습한 후, 단일 작업을 위해 작업별 계층마다 타깃 도메인 데이터세트로 미세 조정하는 방법
# 작업마다 서로 다른 학습 데이터세트를 사용하여 모델을 미세 조정한다.
# 각 계층이 동시에 학습되므로 하나의 작업에 과대적합 되지 않아 일반화된 모델을 얻을 수 있다.


### 변환적 전이 학습(Transductive Transfer Learning)
# 소스 도메인과 타깃 도메인이 유사하지만 완전히 동일하지 않은 경우, 소스 도메인은 레이블이 존재, 타겟 도메인을 레이블이 존재하지 않은 경우 사용
# 레이블이 지정 된 소스도메인으로 사전 학습된 모델을 구축, 레이블이 지정 되지 않은 타겟 도메인으로 모델을 미세 조정해 특정 작업에 대한 성능을 향상 시킴.
# 도메인 적응과 표본 선택 편향/공변량 이동 으로 나뉜다.

# 도메인 적응
# 소스 도메인과 타깃 도메인의 특징 분포를 전이시키는 방법
# 서로 다른 도메인들의 특징 분포를 고려해 학습, 도메인 변화를 확인해 전이

# 표본 선택 편향/공변량 이동
# 소스 도메인과 타깃 도메인의 분산과 편향이 크게 다를 때 표본을 선택해 편향이나 공변량을 이동시키는 방법


### 비지도 전이 학습(Unsupervised Transfer Learning)
# 소스 도메인과 타깃 도메인 모두 레이블이 지정된 데이터가 없는 전이 학습 방법, 소스 도메인에서 타깃 도메인의 성능을 개선하는데 사용할 수 있는 특징 표현을 학습
# 레이블이 없는 전체 데이터로 학습해 데이터가 가진 특징과 특성을 구분할 수 있게 사전 학습된 모델을 구축, 소규모의 레이블이 지정된 데이터를 활용해 미세 조정


### 자연어 처리 분야에서 사용되는 사전 학습된 모델
# Word2Vec,fastText,Bert 등이 있다
# 컴퓨터 비전 : ResNet-50, VGG-16


### 제로-샷 전이 학습(Zero-shot Transfer Learning)
# 사전 학습된 모델을 이용해 다른 도메인에서도 적용할 수 있는 전이 학습 기법 중 하나
# ex) (독수리,새), (참새,새),(오리,새)와 같은 데이터로 모델을 학습한 후 '부엉이'와 같은 새로운 이미지를 분류하는 작업



### 원-샷 전이 학습(One-shot Transfer Learning)
# 제로-샷 학습과 유사하지만 한 번에 하나의 샘플만 사용해 모델을 학습하는 방법, 매우 적은 양의 데이터를 이용하여 분류 문제를 해결할 수 있음
# 서포트 셋(Support Set)과 쿼리 셋(Query Set)을 가정
# 서포트 셋 : 학습에 사용될 클래스의 대표 샘플
# 쿼리 셋 : 새로운 클래스를 분류하기 위한 입력 데이터, 분류 대상 데이터, 서포트 셋에서 수집한 샘플과는 다른 샘플
# 서포트 셋의 대표 샘플과 쿼리 셋 간의 거리를 측정하여 쿼리 셋과 가장 가까운 서포트 셋의 대표 샘플의 클래스로 분류
# ex) 개 클래스, 고양이 클래스 에서 각각 하나의 대표 샘플을 뽑아 서포트 셋을 생성, 분류 대상인 새로운 사진을 쿼리 셋으로 지정, 쿼리 셋과 각각의 서포트 셋의 거리를 측정하여 가장 가까운 서포트 셋의 클래스로 쿼리셋의 클래스를 지정


### 특징 추출(Feature Extraction) 및 미세 조정(Fine-tuning)
# 대규모 데이터세트로 사전 학습된 모델을 작은 데이터세트로 추가 학습해 가중치나 편향을 수정

# 특징 추출
# 특징 추출은 타깃 도메인이 소스 도메인과 유사하고 타깃 도메인의 데이터세트가 적을 때 사용, 특징 추출 계층은 동결해 학습하지 않고 기존에 학습된 모델의 가중치를 사용한다. 대신 모델의 분류기만 재구성해 학습한다.
# 따라서, 새로운 데이터셋에서는 이미지의 카테고리를 결정하는 부분인 마지막 완전 연결층만 학습하고, 나머지 합성곱층 계층들은 사전 훈련된 가중치를 그대로 사용하여 학습되지 않도록 고정한다.
# https://resultofeffort.tistory.com/101

# 미세 조정
# 특징 추출 계층을 일부만 동결하거나 동결하지 않고 타깃 도메인에 대한 학습을 진행

# 특징 추출은 데이터가 적고 유사도가 높은 경우에 사용하는데, 데이터가 적고 유사도 높기 때문에 데이터의 합성곱 층은 학습하지 않는다. (분류기만 학습)

# 미세 조정은 데이터가 많고 유사도가 낮은 경우에 사용하는데, 데이터가 많고 유사도가 낮기 때문에 분류기를 포함한 모델의 매개변수도 다시 학습해야 한다.

# 만약 데이터가 적고 유사도도 낮다면 타깃 성능을 최대한 끌려올리기 위해 저수준 특징 추출 기능을 동결하고 나머지 계층과 분류기를 학습한다.

# 만약 데이터도 많고 유사도도 높다면 상위 계층과 분류기를 학습한다.