### 정칙화(Regularization)
# 과대적합 문제를 방지하기 위한 기술
# 모델이 '암기'가 아니라 '일반화' 할 수 있도록 손실 함수에 규제를 가함
# 암기는 모델이 데이터의 특성이나 패턴을 학습하는 것이 아니라 '데이터 노이즈'를 학습했을 때 발생
# 일반화란 모델이 새로운 데이터에서도 정확한 예측을 할 수 있음을 의미
# 정칙화는 노이즈에 강건하고 일반화된 모데을 구축하기 위해 사용되는 방법
# 정칙화를 적용하면 모델의 분산 값이 낮아진다. 즉, 의존하는 특징의 수가 줄어든다.(일반화)
# 모델이 비교적 복잡하고 데이터 수가 적을 때 주로 사용된다.


### L1 정칙화(Lasso Regularization)
# L1 노름(L1 Norm) 방식을 사용하여 규제하는 방법
# L1 노름은 벡터 또는 행렬값의 절댓값을 계산, 손실 함수에 가중치 절댓값의 합을 추가해 과대적합을 방지한다.
# 모델은 추가된 값(가중치 절댓값의 합)도 최소가 되는 방향으로 학습이 진행된다.
# 학습시 작은 가중치 들은 0으로 수렴하게 되어 '특징 선택'효과를 얻을 수 있다.
# '람다'는 '규제강도'로 0에 가까워지면 더 많은 특징이 사용되기에 과대적합, 람다 값이 커지면 가중치의 값이 0으로 수렴되기에 과소적합 문제가 발생할 수 있다.
# 불필요한 특징은 처리하지 않으므로 모델의 성능이 올라가지만 특징의 수가 줄어들어 정보의 손실이 올 수 있음

### L1 정칙화 적용 방식

# for x,y in train_dataloader:
#     x = x.to(device)
#     y = y.to(device)

#     output = model(x)

#     _labmda = 0.5
#     l1_loss = sum(p.abs().sum() for p in model.parameters())

#     loss = criterion(output,y) + _labmda * l1_loss

# 모델의 가중치를 모두 계산해 모델을 갱신해야 하므로 계산 복잡도가 높아진다.
# L1 정칙화는 미분할 수 없으므로 역전파시 많은 리소스를 소모한다
# 최적의 람다 값을 찾기 위해 여러번의 학습 횟수가 필요하다



### L2 정칙화(L2 Regularization == Ridge Regularization)
# L2 노름(L2 Norm) 방식을 사용하여 규제하는 방법
# L2 노름은 벡터 또는 행렬 값의 크기를 계산, 손실 함수에 가중치 제곱의 합을 추가해 과대적합을 방지한다.
# 하나의 특징이 너무나 중요한 요소가 되지 않도록 규제를 가하는데에 초점
# L1 정칙화에 비해 가중치 값들이 비교적 균일하게 분포하며, 가중치를 0으로 만드는 것이 아닌 0에 가깝게 만든다.

# L1 정칙화는 몇몇 중요한 가중치를 남길 수 있으며
# L2 정칙화는 전체적으로 가중치를 작아지게하여 과적합을 방지할 수 있다.

### L2 정칙화 적용 방식

# for x,y in train_dataloader:
#     x = x.to(device)
#     y = y.to(device)

#     output = model(x)

#     _lambda = 0.5
#     l2_loss = sum(p.pow(2.0).sum() for p in model.parameters())

#     loss = criterion(output,y) + _lambda * l2_loss

# 모델의 가중치를 모두 계산해 모델을 갱신해야 하므로 계산 복잡도가 높아진다.
# 최적의 람다 값을 찾기 위해 여러번의 학습 횟수가 필요하다

# L2 정칙화는 주로 심층 신경망 모델에서 사용한다
# "릿지 회귀(Ridge Regression)" == 선형 회귀 모델에서 L2 정칙화를 적용하는 경우


### 가중치 감쇠(Weight Decay)
# 정칙화 방법과 마찬가지로 모델이 더 작은 가중치를 갖도록 손실함수에 규제를 가하는 방법
# 손실 함수에 규제 항을 추가하는 기술 자체를 의미함.
# 파이토치에서는 L2와 동일한 의미로 사용된다.

### 가중치 감쇠 적용 방식
# weight_dacay 하이퍼파라미터를 설정해 구현 가능
# optimizer = torch.optim.SGD(model.parameters(),lr=0.01,weight_decay=0.01)


### 모멘텀(Momentum)
# 경사 하강법 알고리즘의 변형 중 하나로 이전에 이동했던 방향과 기울기의 크기를 고려하여 가중치를 갱신한다.
# 이전 기울기 값의 일부를 현재 기울기 값에 추가해 가중치를 갱신
# 이전 기울기 값에 의해 설정된 방향으로 더 빠르게 이동하게 된다.
# i번째 모멘텀 = (모멘텀 계수)*(i-1번째 모멘텀) + (learning rate)*(loss에 대한 가중치 도함수)
# (다음 가중치) = (현재 가중치) - (i번째 모멘텀)
# 최적화 함수에 하이퍼파라미터 "momentum"을 설정해 구현 가능
# ex) optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)


### 엘라스틱 넷(Elastic-Net)
# L1 정칙화와 L2 정칙화를 결합해 사용하는 방식
# L1 정칙화는 몇몇 중요한 가중치를 남길 수 있으며
# L2 정칙화는 전체적으로 가중치를 작아지게하여 과적합을 방지할 수 있다.
# L1과 L2를 결합함으로써 균형을 맞춘다.
# 혼합비율을 설정해 각각의 가중치를 규제한다. Elastic-Net = a*L1 + (1-a)*L2 (a는 혼합비율)
# 특징의 수가 샘플의 수보다 많을 때 유의미한 결과를 가져옴.(상관관계가 있는 특징을 잘 처리)
# 각 정칙화가 가진 장점을 적절히 가져올 수 있지만 혼합비율을 조절해야 함(더 많은 튜닝), 정칙화 자체가 계산량이 많음...


### 드롭아웃(Dropout)
# 모델의 훈련 과정에서 일부 노드를 일정 비율로 제거하거나 0으로 설정해 과대적합을 방지하는 방법
# 과대적합을 일으키는 요인 중 하나 "동조화(Co-adaptation)"
# 동조화 : 특정 노드의 가중치나 편향이 큰 값을 갖게 되면 다른 노드가 큰 값을 갖는 노드에 의존하는 현상 => 학습속도가 느려지며 성능이 저하됨.
# 동조화를 방지하기 위해 일부 노드를 삭제!
# 충분한 데이터세트와 학습이 없다면 효과적이지 않다
# 충분한 데이터 세트와 비교적 깊은 모델에 적용


### 드롭아웃 적용방식
# nn.Dropout으로 구현
# from torch import nn    

# class Net(nn.Moduel):
#     def __init__(self):
#         super().__init__()
#         self.layer1 = nn.Linear(10,10)
#         self.dropout = nn.Dropout(p=0.5) #각 노드의 제거 여부를 확률적으로 선택
#         self.layer2 = nn.Linear(10,10)

#     def forward(self,x):
#         x = self.layer1(x)
#         x = self.dropout(x)
#         x = self.layer2(x)
#         return x

# 드랍아웃과 배치 정규화를 도잇에 사용하면 안 된다. 서로의 정칙화 효과를 방해할 수 있다.
# 배치 정규화는 내부 공변량을 줄여 과대적합을 방해하는데 드랍아웃이 일부노드를 삭제..... 
# 굳이 사용한다면 드랍아웃 이후 배치 정규화를 사용해야한다.
# 드랍아웃은 학습시에만 적용하며 추론과정에서는 모든 노드를 통해 예측해야 한다.
    

### 그레이디언트 클리핑
# 학습시 기울기가 너무 커지는 현상을 방지하는데 사용하는 기술
# 과대적합은 특정 가중치가 너무 높을 때 발생, 특정 가중치가 너무 높으면 높은 분산을 갖게 되며 성능이 저하됨.
# 가중치의 최대값을 규제해 최대 임계값을 초과하지 않도록 기울기를 잘라 설정한 임계값으로 변경
# 가중치의 노름이 최대 임계값(r) 보다 높은 경우 수행됨.
# 기울기 폭주에 취약한 순환신경망(RNN), LSTM 모델에 적용된다.

### 그레이디언트 클리핑 함수
# import torch

# grad_norm = torch.nn.utils.clip_grad_norm_(
#     parameters,
#     max_norm,
#     norm_type=2.0
# )
# 매개변수,최대노름,노름유형을 매개변수로 받는다.


### 그레이디언트 클리핑 적용 방식

# for x,y in train_dataloader:
#     x = x.to(device)
#     y = y.to(device)

#     output = model(x)
#     loss = criterion(output,y)

#     optimizer.zero_grad()
#     loss.backward()

#     torch.nn.utils.clip_grad_norm_(model.parameters(),0.1)

#     optimizer.step()

# 역전파 수행후 최적화 함수를 반영하기전에 수행된다.(임계값을 초과하는 경우 기울기를 임계값으로 자르기 때문)
# 최대 임계값 설정에 따라 학습이 좌지우지 되므로 여러번 실험을 통해 결정해야 함.